{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('../data/books.csv')[['book_id', 'title', 'authors']]    # TODO: think about using also the columns\n",
    "\n",
    "# df_ratings = pd.read_csv('../data/ratings.csv').sample(500000)  # TODO: remove the sampling on the final run\n",
    "df_ratings = pd.read_csv('../data/ratings.csv')\n",
    "\n",
    "print(df_books.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books features\n",
    "df_books[\"text_to_embed\"] = \"Title: \" + df_books[\"title\"] + \" Authors: \" + df_books[\"authors\"]\n",
    "with torch.no_grad():\n",
    "    titles_emb = model.encode(df_books['text_to_embed'].values, device=device, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "del model\n",
    "torch.cuda.empty_cache()    \n",
    "\n",
    "books_features = torch.tensor(titles_emb)\n",
    "print(\"Books features shape:\", books_features.shape)\n",
    "\n",
    "# Users features: as we don't have any information we will use random features\n",
    "# users_features = torch.rand(df_ratings['user_id'].nunique(), 768, device=device)\n",
    "# print(\"Users features shape:\", users_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding users\n",
    "\n",
    "# # Create a bipartite graph\n",
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "B.add_nodes_from(df_ratings['user_id'].unique(), bipartite=0)  # Users\n",
    "B.add_nodes_from(df_ratings['book_id'].unique(), bipartite=1)  # Books\n",
    "\n",
    "# Add edges between users and books\n",
    "for _, row in tqdm(df_ratings.iterrows(), total=df_ratings.shape[0], desc=\"Adding edges\"):\n",
    "    B.add_edge(row['user_id'], row['book_id'], weight=row['rating'])\n",
    "\n",
    "# Compute metrics\n",
    "centrality = nx.degree_centrality(B)\n",
    "print('degree centrality computed')\n",
    "pagerank = nx.pagerank(B, weight='weight')\n",
    "print('pagerank computed')\n",
    "average_rating = df_ratings.groupby('user_id')['rating'].mean()\n",
    "print('all metrics computed')\n",
    "\n",
    "# # Prepare feature vectors for users\n",
    "features = pd.DataFrame(index=df_ratings['user_id'].unique())\n",
    "features['degree'] = [centrality[node] for node in features.index]\n",
    "features['pagerank'] = [pagerank[node] for node in features.index]\n",
    "features['average_rating'] = [average_rating.get(node, 0) for node in features.index]  # Add average ratings\n",
    "\n",
    "# # Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features), index=features.index, columns=features.columns)\n",
    "\n",
    "# # Display the normalized features\n",
    "users_features = features_scaled.to_numpy(dtype=np.float32)\n",
    "\n",
    "features_scaled.head() \n",
    "\n",
    "# aprox 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes keeping user_id, book_id, rating, title, authors\n",
    "df_ratings = pd.merge(df_ratings, df_books, on='book_id')\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from the user_id to a unique consecutive value in the range [0, num_users]:\n",
    "unique_user_id = df_ratings['user_id'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'user_id': unique_user_id, \n",
    "    'mapped_user_id': pd.RangeIndex(len(unique_user_id))\n",
    "    })\n",
    "print(\"Mapping of user IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_user_id.head())\n",
    "print()\n",
    "\n",
    "# Create a mapping from the book_id to a unique consecutive value in the range [0, num_books]:\n",
    "unique_book_id = df_ratings['book_id'].unique()\n",
    "unique_book_id = pd.DataFrame(data={\n",
    "    'book_id': unique_book_id,\n",
    "    'mapped_book_id': pd.RangeIndex(len(unique_book_id))\n",
    "    })\n",
    "print(\"Mapping of book IDs to consecutive values:\")\n",
    "print(\"===========================================\")\n",
    "print(unique_book_id.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.merge(unique_user_id, on='user_id')\n",
    "df_ratings = df_ratings.merge(unique_book_id, on='book_id')\n",
    "\n",
    "# With this, we are ready to create the edge_index representation in COO format\n",
    "# following the PyTorch Geometric semantics:\n",
    "edge_index = torch.stack([\n",
    "    torch.tensor(df_ratings['mapped_user_id'].values), \n",
    "    torch.tensor(df_ratings['mapped_book_id'].values)]\n",
    "    , dim=0)\n",
    "\n",
    "print(edge_index[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Create the heterogeneous graph data object:\n",
    "data = HeteroData()\n",
    "\n",
    "# Add the user nodes:\n",
    "data['user'].x = torch.tensor(users_features,)  # (num_users, num_users_features)\n",
    "\n",
    "# Add the book nodes:\n",
    "data['book'].x = torch.tensor(titles_emb,)  # (num_books, num_books_features)\n",
    "\n",
    "# Add the rating edges:\n",
    "data['user', 'rates', 'book'].edge_index = edge_index  # (2, num_ratings)\n",
    "\n",
    "# Add the rating labels:\n",
    "rating = torch.from_numpy(df_ratings['rating'].values)\n",
    "data['user', 'rates', 'book'].edge_label = rating  # [num_ratings]\n",
    "\n",
    "# We also need to make sure to add the reverse edges from books to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "data = T.ToUndirected()(data)\n",
    "\n",
    "# With the above transformation we also got reversed labels for the edges.\n",
    "# We remove them\n",
    "del data['book', 'rev_rates', 'user'].edge_label\n",
    "\n",
    "print(data['user'].num_nodes,len(unique_user_id))\n",
    "assert data['user'].num_nodes == len(unique_user_id)\n",
    "assert data['user', 'rates', 'book'].num_edges == len(df_ratings)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    add_negative_train_samples=True,\n",
    "    num_val=0.15,\n",
    "    num_test=0.15,\n",
    "    edge_types=[('user', 'rates', 'book')],\n",
    "    rev_edge_types=[('book', 'rev_rates', 'user')],\n",
    ")(data)\n",
    "train_data, val_data, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['book'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import HGTLoader\n",
    "train_mask = torch.tensor([True] * train_data[\"user\"].x.shape[0], )\n",
    "train_loader = HGTLoader(\n",
    "    train_data,\n",
    "    num_samples=[1024] * 4,\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    input_nodes=(\"user\", train_mask),\n",
    ")\n",
    "val_loader = HGTLoader(\n",
    "    val_data,\n",
    "    num_samples=[1024] * 4,\n",
    "    shuffle=False,\n",
    "    batch_size=128,\n",
    "    input_nodes=(\"user\", torch.tensor([True] * val_data[\"user\"].x.shape[0], )),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'book'].edge_label_index)\n",
    "        loss = criterion(pred, batch['user', 'rates', 'book'].edge_label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / (len(data_loader.dataset) / 128)\n",
    "\n",
    "# Testing Loop\n",
    "def test(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch.x_dict, batch.edge_index_dict, batch['user', 'rates', 'book'].edge_label_index)\n",
    "            loss = criterion(pred, batch['user', 'rates', 'book'].edge_label.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / (len(data_loader.dataset) / 128)\n",
    "\n",
    "# Main training and testing routines\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = test(model, val_loader, criterion)\n",
    "    \n",
    "    # val_loss = test(model, val_data, criterion)\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    val_losses.append(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "# Optionally, after training, you can evaluate your model on the test dataset\n",
    "# test_loss = test(model, test_data, criterion)\n",
    "# print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    if device != 'cpu':\n",
    "        data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred = model(data.x_dict, data.edge_index_dict, data['user', 'rates', 'book'].edge_label_index)\n",
    "    loss = criterion(pred, data['user', 'rates', 'book'].edge_label.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss = loss.item()\n",
    "    return total_loss \n",
    "\n",
    "# Testing Loop\n",
    "def test(model, data, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if device != 'cpu':\n",
    "            data = data.to(device)\n",
    "        pred = model(data.x_dict, data.edge_index_dict, data['user', 'rates', 'book'].edge_label_index)\n",
    "        loss = criterion(pred, data['user', 'rates', 'book'].edge_label.float())\n",
    "        total_loss = loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# Main training and testing routines\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for d in train_loader:\n",
    "        train_loss = train(model, train_data, optimizer, criterion)\n",
    "        val_loss = test(model, val_data, criterion)\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(val_loss)\n",
    "\n",
    "# Optionally, after training, you can evaluate your model on the test dataset\n",
    "test_loss = test(model, test_data, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, test_data, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=10).to(device)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_review = model(test_data.x_dict, test_data.edge_index_dict, test_data['user', 'rates', 'book'].edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(pred_review.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['user', 'rates', 'book'].edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['user', 'rates', 'book'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to numpy arrays\n",
    "user_ids_np = test_data['user', 'rates', 'book'].edge_label_index[0].numpy()\n",
    "book_ids_np = test_data['user', 'rates', 'book'].edge_label_index[1].numpy()\n",
    "ratings_np = test_data['user', 'rates', 'book'].edge_label.numpy()\n",
    "ratings_pred_np = pred_review.detach().numpy()\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'user_id': user_ids_np,\n",
    "    'book_id': book_ids_np,\n",
    "    'rating': ratings_np, \n",
    "    'predicted_rating': ratings_pred_np\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_ratings = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from evaluation_metrics import *\n",
    "\n",
    "k = 10\n",
    "top_k_recommendations = get_top_k_recommendations(df_ratings, k)\n",
    "actual_items = get_actual_items(df_ratings) # ground truth\n",
    "\n",
    "# Evaluate the recommendations\n",
    "mean_precision, mean_recall, mean_f1 = evaluate_recommendations(top_k_recommendations, actual_items, k)\n",
    "print(f\"Mean Precision@{k}: {mean_precision}\")\n",
    "print(f\"Mean Recall@{k}: {mean_recall}\")\n",
    "print(f\"Mean F1 Score@{k}: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix factorization:\n",
    "\n",
    "Mean Precision@10: 0.7722234424908242\n",
    "Mean Recall@10: 0.5475533441372822\n",
    "Mean F1 Score@10: 0.6128487333956821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo\n",
    "visualization on how the data looks like\n",
    "\n",
    "\n",
    "report \n",
    "objective and motivation \n",
    "analysis of the data\n",
    "method: improving over matrix factorization baseline\n",
    "results\n",
    "future study: even an idea about how to use diversity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
