{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('../data/books.csv')[['book_id', 'title', 'authors']]    # TODO: think about using also the columns\n",
    "\n",
    "# df_ratings = pd.read_csv('../data/ratings.csv').sample(500000)  # TODO: remove the sampling on the final run\n",
    "df_ratings = pd.read_csv('../data/ratings.csv')\n",
    "\n",
    "print(df_books.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books features\n",
    "df_books[\"text_to_embed\"] = \"Title: \" + df_books[\"title\"] + \" Authors: \" + df_books[\"authors\"]\n",
    "with torch.no_grad():\n",
    "    titles_emb = model.encode(df_books['text_to_embed'].values, device=device, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "del model\n",
    "torch.cuda.empty_cache()    \n",
    "\n",
    "books_features = torch.tensor(titles_emb)\n",
    "print(\"Books features shape:\", books_features.shape)\n",
    "\n",
    "# Users features: as we don't have any information we will use random features\n",
    "# users_features = torch.rand(df_ratings['user_id'].nunique(), 768, device=device)\n",
    "# print(\"Users features shape:\", users_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding users\n",
    "\n",
    "# # Create a bipartite graph\n",
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "B.add_nodes_from(df_ratings['user_id'].unique(), bipartite=0)  # Users\n",
    "B.add_nodes_from(df_ratings['book_id'].unique(), bipartite=1)  # Books\n",
    "\n",
    "# Add edges between users and books\n",
    "for _, row in tqdm(df_ratings.iterrows(), total=df_ratings.shape[0], desc=\"Adding edges\"):\n",
    "    B.add_edge(row['user_id'], row['book_id'], weight=row['rating'])\n",
    "\n",
    "# Compute metrics\n",
    "centrality = nx.degree_centrality(B)\n",
    "print('degree centrality computed')\n",
    "pagerank = nx.pagerank(B, weight='weight')\n",
    "print('pagerank computed')\n",
    "average_rating = df_ratings.groupby('user_id')['rating'].mean()\n",
    "print('all metrics computed')\n",
    "\n",
    "# # Prepare feature vectors for users\n",
    "features = pd.DataFrame(index=df_ratings['user_id'].unique())\n",
    "features['degree'] = [centrality[node] for node in features.index]\n",
    "features['pagerank'] = [pagerank[node] for node in features.index]\n",
    "features['average_rating'] = [average_rating.get(node, 0) for node in features.index]  # Add average ratings\n",
    "\n",
    "# # Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features), index=features.index, columns=features.columns)\n",
    "\n",
    "# # Display the normalized features\n",
    "users_features = features_scaled.to_numpy(dtype=np.float32)\n",
    "\n",
    "features_scaled.head() \n",
    "\n",
    "# aprox 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes keeping user_id, book_id, rating, title, authors\n",
    "df_ratings = pd.merge(df_ratings, df_books, on='book_id')\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from the user_id to a unique consecutive value in the range [0, num_users]:\n",
    "unique_user_id = df_ratings['user_id'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'user_id': unique_user_id, \n",
    "    'mapped_user_id': pd.RangeIndex(len(unique_user_id))\n",
    "    })\n",
    "print(\"Mapping of user IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_user_id.head())\n",
    "print()\n",
    "\n",
    "# Create a mapping from the book_id to a unique consecutive value in the range [0, num_books]:\n",
    "unique_book_id = df_ratings['book_id'].unique()\n",
    "unique_book_id = pd.DataFrame(data={\n",
    "    'book_id': unique_book_id,\n",
    "    'mapped_book_id': pd.RangeIndex(len(unique_book_id))\n",
    "    })\n",
    "print(\"Mapping of book IDs to consecutive values:\")\n",
    "print(\"===========================================\")\n",
    "print(unique_book_id.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.merge(unique_user_id, on='user_id')\n",
    "df_ratings = df_ratings.merge(unique_book_id, on='book_id')\n",
    "\n",
    "# With this, we are ready to create the edge_index representation in COO format\n",
    "# following the PyTorch Geometric semantics:\n",
    "edge_index = torch.stack([\n",
    "    torch.tensor(df_ratings['mapped_user_id'].values), \n",
    "    torch.tensor(df_ratings['mapped_book_id'].values)]\n",
    "    , dim=0)\n",
    "\n",
    "print(edge_index[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Create the heterogeneous graph data object:\n",
    "data = HeteroData()\n",
    "\n",
    "# Add the user nodes:\n",
    "data['user'].x = torch.tensor(users_features,)  # (num_users, num_users_features)\n",
    "\n",
    "# Add the book nodes:\n",
    "data['book'].x = torch.tensor(titles_emb,)  # (num_books, num_books_features)\n",
    "\n",
    "# Add the rating edges:\n",
    "data['user', 'rates', 'book'].edge_index = edge_index  # (2, num_ratings)\n",
    "\n",
    "# Add the rating labels:\n",
    "rating = torch.from_numpy(df_ratings['rating'].values)\n",
    "data['user', 'rates', 'book'].edge_label = rating  # [num_ratings]\n",
    "\n",
    "# We also need to make sure to add the reverse edges from books to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "data = T.ToUndirected()(data)\n",
    "\n",
    "# With the above transformation we also got reversed labels for the edges.\n",
    "# We remove them\n",
    "del data['book', 'rev_rates', 'user'].edge_label\n",
    "\n",
    "print(data['user'].num_nodes,len(unique_user_id))\n",
    "assert data['user'].num_nodes == len(unique_user_id)\n",
    "assert data['user', 'rates', 'book'].num_edges == len(df_ratings)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    add_negative_train_samples=False,\n",
    "    num_val=0.15,\n",
    "    num_test=0.15,\n",
    "    edge_types=[('user', 'rates', 'book')],\n",
    "    rev_edge_types=[('book', 'rev_rates', 'user')],\n",
    ")(data)\n",
    "train_data, val_data, test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NOTE:**\n",
    "To be more specific:\n",
    "* At **training** time:\n",
    "  * $\\text{training supervision edges} = \\text{training message passing edges}$ <br>\n",
    "* At **validation** time:\n",
    "  * $\\text{validation message passing edges} = \\text{training message passing edges} = \\text{training supervision edges}$ <br>\n",
    "  * $\\text{validation supervision edges} \\notin \\text{training supervision edges}$: disjoint with training supervision edges <br>\n",
    "\n",
    "* At **test** time:\n",
    "  * $\\text{test message passing edges} = \\text{validation supervison edges} + \\text{training supervision edges}$ <br>\n",
    "  * $\\text{test supervision edges} \\notin \\lbrace \\text{training supervision edges}, \\text{valid supervision edges} \\rbrace$: disjoint with training supervision edges and validation supervision edges. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch import Tensor\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Takes the edge_index (not the edge_label_index) as input, and performs\n",
    "        # message passing on the graph.\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x_user: Tensor, x_book: Tensor, edge_label_index):     \n",
    "        # NEW VERSION (simplified)\n",
    "        row, col = edge_label_index\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (x_user[row] * x_book[col]).sum(dim=-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, data: HeteroData):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.book_lin = torch.nn.Linear(384, hidden_channels)\n",
    "        self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "        self.book_emb = torch.nn.Embedding(data[\"book\"].num_nodes, hidden_channels)\n",
    "        \n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, data: HeteroData):\n",
    "        # This is completely ignoring the user features and only caring about the book features.\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(data[\"user\"].n_id),\n",
    "          \"book\": self.book_lin(data[\"book\"].x) + self.book_emb(data[\"book\"].n_id),\n",
    "        }\n",
    "                \n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        x_dict = self.encoder(x_dict, data.edge_index_dict)\n",
    "        return self.decoder(\n",
    "            x_dict[\"user\"],\n",
    "            x_dict[\"book\"],\n",
    "            data[\"user\", \"rates\", \"book\"].edge_label_index,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=64, data=data).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Define seed edges:\n",
    "edge_label_index = train_data[\"user\", \"rates\", \"book\"].edge_label_index\n",
    "edge_label = train_data[\"user\", \"rates\", \"book\"].edge_label\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,  # TODO\n",
    "    num_neighbors=[50, 50],  # TODO\n",
    "    neg_sampling_ratio=2,  # TODO\n",
    "    edge_label_index=((\"user\", \"rates\", \"book\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logging_steps = 15\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    total_loss = total_examples = 0\n",
    "    \n",
    "    est_loss = float(\"inf\")\n",
    "    pbar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch:03d} - Estimated loss {est_loss}\", total=len(train_loader))\n",
    "    \n",
    "    for i, sampled_data in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = sampled_data.to(device)\n",
    "        pred = model.forward(batch)\n",
    "        loss = F.mse_loss(pred, batch[\"user\", \"rates\", \"book\"].edge_label.to(torch.float32))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.numel()\n",
    "        total_examples += pred.numel()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if i % logging_steps == 0:\n",
    "            est_loss = total_loss / total_examples\n",
    "            pbar.set_description(f\"Epoch {epoch:03d} - Estimated loss {est_loss}\")\n",
    "            train_losses.append(est_loss)\n",
    "            \n",
    "    # Run Validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_valid_loss = total_valid_examples = 0\n",
    "        for batch in tqdm(val_data, desc=f\"Validation {epoch:03d}\"):\n",
    "            batch = batch.to(device)\n",
    "            pred = model.forward(batch)\n",
    "            loss = F.mse_loss(pred, batch[\"user\", \"rates\", \"book\"].edge_label.to(torch.float32))\n",
    "            \n",
    "            total_valid_loss += float(loss) * pred.numel()\n",
    "            total_valid_examples += pred.numel()\n",
    "        valid_loss = total_valid_loss / total_valid_examples\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f\"Validation loss: {valid_loss}\")\n",
    "        model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
